{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word Representations in Natural Language Processing\n",
    "This notebook shows examples of three different types of word representations in NLP: Bag of Words (BoW), TF-IDF, and Word2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\kinla\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Required Libraries\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from gensim.models import Word2Vec\n",
    "nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag of words (BoW)\n",
    "\n",
    "Interpreting the Output of BoW\n",
    "Bag of Words (BoW) model's output has two parts:\n",
    "\n",
    "Vocabulary: This is a dictionary where the keys are the unique words in the text, and the values are their respective identifiers. Each word in the text is assigned a unique identifier.\n",
    "\n",
    "Encoded Document: This is a matrix where the rows represent the documents and columns represent the unique words in the vocabulary. The values in the matrix are the counts of each word in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "Encoded Document:  [[1 1 1 1 1 1 1 2]]\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "# Create the Transform\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# Summarize\n",
    "print('Vocabulary: ', vectorizer.vocabulary_)\n",
    "print('Encoded Document: ', vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TF-IDF\n",
    "\n",
    "Interpreting the Output of TF-IDF\n",
    "TF-IDF model's output is similar to the BoW model but instead of raw counts, it contains the TF-IDF scores.\n",
    "\n",
    "Vocabulary: This is again a dictionary where the keys are the unique words in the text, and the values are their respective identifiers.\n",
    "\n",
    "Encoded Document: This is a matrix where the rows represent the documents and columns represent the unique words in the vocabulary. The values in the matrix are the TF-IDF scores of each word in each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary:  {'the': 7, 'quick': 6, 'brown': 0, 'fox': 2, 'jumped': 3, 'over': 5, 'lazy': 4, 'dog': 1}\n",
      "Encoded Document:  [[0.30151134 0.30151134 0.30151134 0.30151134 0.30151134 0.30151134\n",
      "  0.30151134 0.60302269]]\n"
     ]
    }
   ],
   "source": [
    "# Example text\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "\n",
    "# Create the Transform\n",
    "vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Tokenize and build vocab\n",
    "vectorizer.fit(text)\n",
    "\n",
    "# Encode the Document\n",
    "vector = vectorizer.transform(text)\n",
    "\n",
    "# Summarize\n",
    "print('Vocabulary: ', vectorizer.vocabulary_)\n",
    "print('Encoded Document: ', vector.toarray())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2Vec\n",
    "\n",
    "Interpreting the Output of Word2Vec\n",
    "The output of a Word2Vec model is a bit different from the previous two:\n",
    "\n",
    "Model's Vocabulary: This is a dictionary where keys are unique words in the text and the values are a bunch of information about the word, including its count, index, and vector representation.\n",
    "\n",
    "Vector for a word: Word2Vec model represents each word as a high-dimensional vector (300 in this example). These vectors capture deep semantic meanings and relations with other words. For example, similar words have similar vectors, and the model can understand analogies between words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model's Vocabulary:  {'.': 0, 'dog': 1, 'lazy': 2, 'the': 3, 'over': 4, 'jumped': 5, 'fox': 6, 'brown': 7, 'quick': 8, 'The': 9}\n",
      "Vector for \"fox\": [ 3.2451833e-03 -3.2601277e-03 -2.1664966e-03  9.2792866e-04\n",
      "  2.1439958e-03 -1.7891225e-03  9.1749750e-04  3.0404369e-03\n",
      " -2.2718073e-03 -2.0333040e-03 -1.6632139e-03 -1.2254707e-03\n",
      "  6.1657192e-04  3.2275442e-03  2.1459253e-03  1.3236403e-04\n",
      "  8.2358957e-04  2.8134971e-03  3.0429931e-03  1.8762510e-03\n",
      "  1.9820877e-03 -2.5402287e-03 -1.2758906e-03 -1.8934441e-03\n",
      "  2.0605915e-03 -7.5214985e-04 -2.9264784e-03  2.5397083e-03\n",
      "  2.7998944e-03 -1.1067450e-03  3.0388865e-03 -2.4611989e-04\n",
      " -1.2088398e-03 -1.2823025e-04  6.4808926e-05 -1.1682988e-03\n",
      "  9.3774719e-04  1.9099017e-03  2.2896698e-03 -2.9678221e-03\n",
      " -7.3090952e-04 -1.8272658e-03  2.5070333e-03  2.1672344e-03\n",
      " -1.4535740e-03  7.7561103e-04 -1.9845536e-03  7.8831908e-05\n",
      "  3.1539197e-03 -8.6994725e-04 -1.7292392e-03 -2.4657364e-03\n",
      " -9.7064732e-04 -2.8810461e-04  1.1759539e-03  3.2472964e-03\n",
      " -1.1297583e-03  6.3392281e-04  3.2270029e-03  5.1052886e-04\n",
      "  3.2883207e-04  3.2674570e-03  3.0984867e-03  2.5693555e-03\n",
      " -2.0568422e-03  3.3279951e-03  1.9496623e-03  3.0242221e-03\n",
      " -6.6506624e-04  1.1166453e-03  2.2778523e-03 -1.2979190e-03\n",
      "  2.2142834e-03  8.5428753e-04  3.1045766e-03 -1.0119347e-03\n",
      " -1.0364564e-03  2.0717967e-03 -3.0260817e-03 -2.4179972e-03\n",
      " -2.1666754e-03 -2.4969139e-04 -7.8767381e-04  2.2718399e-03\n",
      "  3.0788640e-03 -3.0325295e-04  4.7094145e-04  6.7345222e-04\n",
      " -6.7326624e-04 -2.6781138e-03  2.4803500e-03 -1.4326338e-03\n",
      "  1.5255061e-03  3.0299013e-03  1.0144075e-03  1.0462634e-03\n",
      "  1.3539429e-03 -9.0040563e-04  1.2749231e-03  1.1254112e-04\n",
      "  1.8755706e-03  1.8324570e-03  6.0970662e-04  1.9164690e-03\n",
      " -2.9893592e-03  2.1864525e-03  3.0753308e-03 -1.4023825e-03\n",
      "  5.3585012e-04 -1.7446271e-03  3.5273950e-04  9.2338957e-04\n",
      "  2.7202456e-03  1.8133760e-04  8.5235276e-04  4.3257832e-04\n",
      "  2.8008409e-03 -1.9025676e-03 -2.0872767e-03 -1.2091728e-03\n",
      " -7.6684990e-04  1.6803542e-03 -2.7067857e-03 -9.4451191e-04\n",
      " -2.7324755e-03  1.7165701e-03 -8.5602124e-04 -3.0223692e-03\n",
      "  1.3572430e-03  3.0057745e-03 -1.0125533e-03 -1.9461799e-03\n",
      "  1.0066294e-03 -1.4528274e-04 -3.3264789e-03  2.8059015e-03\n",
      " -2.4462959e-03 -1.6434690e-03 -8.8569365e-04 -1.8174382e-03\n",
      "  5.7217001e-04  3.2376046e-03  1.5240908e-03  2.6962010e-03\n",
      " -1.5681943e-04  2.1497448e-04 -8.8945072e-04 -2.9265205e-03\n",
      "  1.1437678e-03  6.9779117e-04 -3.1406181e-03 -1.6561457e-03\n",
      " -3.2446997e-03 -1.9065972e-03  1.3548473e-03  2.8809535e-03\n",
      "  1.3705500e-03  7.9615472e-04  2.7149259e-03 -3.7306984e-04\n",
      " -4.6590448e-04 -2.9156078e-03 -4.1930674e-05 -8.5585751e-04\n",
      "  1.2869239e-04  2.4265540e-03 -2.3471534e-03 -1.3154916e-03\n",
      " -2.2215350e-03 -1.1813716e-03 -1.1052771e-03  7.1237364e-04\n",
      "  1.1093895e-03 -1.6523957e-03 -1.5154302e-03  3.7956476e-04\n",
      "  1.8178276e-03  1.7912165e-03 -9.8951219e-04 -1.4221752e-03\n",
      " -1.8722157e-03 -1.8166105e-04  6.4879097e-04  5.0844869e-04\n",
      "  2.4508433e-03 -9.1112417e-04 -2.1974643e-05 -1.8425445e-03\n",
      " -3.9002180e-04 -2.5706545e-03 -3.1977653e-04  4.3655833e-04\n",
      " -2.8649147e-03  2.9161945e-03 -3.0692888e-03 -3.2082256e-03\n",
      " -2.8372081e-03  2.4377562e-03  1.8218561e-03  3.0831539e-03\n",
      "  8.5672579e-04  2.8217633e-04 -8.4648014e-04  3.1192228e-03\n",
      "  9.1916957e-04  1.3652162e-03 -3.9431173e-04  3.0150136e-04\n",
      "  2.2081025e-03 -2.4216692e-04  1.1138360e-03 -2.2400697e-04\n",
      "  1.7494658e-03  1.2133113e-03  8.6084328e-04 -1.7699644e-03\n",
      " -1.5693669e-03  1.4360074e-03 -1.9690569e-03 -6.0068767e-05\n",
      " -2.1117210e-04  1.1644145e-03 -2.8140887e-03  2.9390780e-03\n",
      " -4.8291485e-04 -1.7769758e-03  1.3514896e-03 -6.4516347e-04\n",
      " -2.5884942e-03 -1.4989328e-03 -1.2924234e-04 -2.9821575e-03\n",
      "  1.9018770e-04  8.1393123e-04 -1.0750910e-03  8.5680006e-04\n",
      "  8.2693261e-04  3.3294053e-03  4.7683518e-04  6.7351264e-04\n",
      "  9.2652044e-04 -6.9262384e-04 -2.8995387e-03  2.6743913e-03\n",
      " -6.5823237e-04 -3.2307713e-03 -2.1840015e-03 -1.3152870e-03\n",
      "  1.3181157e-03  1.6795270e-03  2.0286704e-03 -2.2567534e-03\n",
      "  2.2961259e-04 -9.2509389e-04 -1.7363989e-03  2.3270908e-03\n",
      "  1.3173223e-03 -1.0355023e-03 -2.7589691e-03 -1.7135954e-03\n",
      " -2.1651149e-04  2.6039216e-03  2.0147285e-03 -2.8172187e-03\n",
      " -3.1878452e-03  2.3779452e-03 -7.7585538e-04 -1.2307696e-03\n",
      "  1.9166108e-03 -1.9481019e-03  1.6979850e-03 -8.0808401e-05\n",
      " -2.2917211e-03 -1.1054595e-04  2.1196639e-03  3.0979796e-03\n",
      "  7.4030441e-04  1.6833643e-03 -1.6581746e-03 -2.6585063e-04\n",
      " -1.7725452e-03  3.9632083e-04 -5.9773802e-04 -1.2108783e-03\n",
      " -2.3381265e-03  3.2181437e-03  9.9194248e-04 -7.6040823e-04\n",
      " -1.3946967e-03  2.5712624e-03 -2.1606183e-03  1.0405143e-03\n",
      "  2.6245238e-04  2.7742935e-03  2.2787256e-03 -9.6954900e-04\n",
      "  8.4503333e-04 -5.5535755e-04 -3.1515539e-03 -8.7115366e-04]\n"
     ]
    }
   ],
   "source": [
    "# Preparing the text\n",
    "text = [\"The quick brown fox jumped over the lazy dog.\"]\n",
    "tokenized_text = [nltk.word_tokenize(sent) for sent in text]\n",
    "\n",
    "# Creating the model and setting values for the various parameters\n",
    "num_features = 300  # Word vector dimensionality\n",
    "min_word_count = 1  # Minimum word count\n",
    "num_workers = 4     # Number of parallel threads\n",
    "context = 10        # Context window size\n",
    "\n",
    "# Initializing the train model\n",
    "model = Word2Vec(\n",
    "    tokenized_text,\n",
    "    workers=num_workers,\n",
    "    vector_size=num_features,  # Change 'size' to 'vector_size'\n",
    "    min_count=min_word_count,\n",
    "    window=context,\n",
    ")\n",
    "\n",
    "# Accessing the model's vocabulary\n",
    "# `model.wv` represents word vector\n",
    "print('Model\\'s Vocabulary: ', model.wv.key_to_index)  # Update 'vocab' to 'key_to_index'\n",
    "\n",
    "# Access vector for one word\n",
    "print('Vector for \"fox\":', model.wv['fox'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
