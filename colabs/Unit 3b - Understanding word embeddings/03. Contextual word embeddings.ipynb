{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT\n",
    "\n",
    "BERT is an example of contextual embeddings, usually involves more complex models and libraries than traditional bag-of-words models like TF-IDF. The transformers library by Hugging Face is a popular tool for working with pre-trained contextual embeddings models, including BERT."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this script, we utilize the Hugging Face Transformers library to work with BERT, a pre-trained transformer-based language model. We start by loading a pre-trained BERT tokenizer and model. \n",
    "\n",
    "The example text, \"The quick brown fox jumped over the lazy dog,\" is then tokenized using the tokenizer, and the resulting tokens are passed to the BERT model to obtain embeddings for each token. \n",
    "\n",
    "The embeddings for the special [CLS] token, which can be used as a sentence embedding, are extracted from the model's output. \n",
    "\n",
    "Finally, the script prints the tokenized text and the BERT embeddings for each token, providing a comprehensive overview of the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import BertTokenizer, TFBertModel\n",
    "\n",
    "# Example text\n",
    "text = \"The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "# Load pre-trained BERT tokenizer and model\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = TFBertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Tokenize the input text\n",
    "tokens = tokenizer(text, return_tensors='tf')\n",
    "\n",
    "# Get BERT embeddings for each token\n",
    "outputs = model(tokens)\n",
    "\n",
    "# Extract the embeddings for the [CLS] token (which can be used as a sentence embedding)\n",
    "embeddings = outputs.last_hidden_state[:, 0, :]\n",
    "\n",
    "# Convert to a NumPy array for easy printing\n",
    "embeddings_array = embeddings.numpy()\n",
    "\n",
    "# Summarize\n",
    "print('Tokenized Text:', tokens)\n",
    "print('BERT Embeddings for each token:', embeddings_array)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating contextual embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy.spatial.distance import cosine\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "\n",
    "# Define word pairs\n",
    "word_pairs = [(\"paris\", \"france\"), (\"berlin\", \"germany\")]\n",
    "\n",
    "# Get embeddings for each word\n",
    "embeddings = {}\n",
    "for pair in word_pairs:\n",
    "    for word in pair:\n",
    "        tokens = tokenizer(word, return_tensors='tf')\n",
    "        outputs = model(tokens)\n",
    "        embeddings[word] = outputs.last_hidden_state[:, 0, :].numpy().flatten()\n",
    "\n",
    "# Compute distances\n",
    "for pair in word_pairs:\n",
    "    dist = cosine(embeddings[pair[0]], embeddings[pair[1]])\n",
    "    print(f\"Cosine distance between '{pair[0]}' and '{pair[1]}':\", dist)\n",
    "\n",
    "# Visualize in 3D space\n",
    "pca = PCA(n_components=3)\n",
    "embedding_values = list(embeddings.values())\n",
    "embedding_values_3d = pca.fit_transform(embedding_values)\n",
    "\n",
    "fig = plt.figure(figsize=(8, 8))\n",
    "ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "# Add points\n",
    "for i, word in enumerate(embeddings.keys()):\n",
    "    ax.scatter(embedding_values_3d[i, 0], embedding_values_3d[i, 1], embedding_values_3d[i, 2], label=word)\n",
    "\n",
    "# Add lines\n",
    "for pair in word_pairs:\n",
    "    points = [embedding_values_3d[i, :] for i, word in enumerate(embeddings.keys()) if word in pair]\n",
    "    ax.plot([points[0][0], points[1][0]],\n",
    "            [points[0][1], points[1][1]],\n",
    "            [points[0][2], points[1][2]])\n",
    "\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
